# ä¸­æ–‡CLIPæ¨¡å‹æ”¯æŒæ–¹æ¡ˆ

**æ—¥æœŸ**: 2025-11-26
**ç‰ˆæœ¬**: v1.0
**çŠ¶æ€**: ğŸ“‹ è®¾è®¡é˜¶æ®µ

---

## ğŸ“Œ ç›®æ ‡

ä¸º VIndex æ·»åŠ **ä¸­æ–‡CLIPæ¨¡å‹æ”¯æŒ**ï¼Œå®ç°ï¼š
- âœ… ä¸­æ–‡æ–‡æœå›¾ï¼ˆ"ä¸€åªçŒ«"ï¼‰
- âœ… ä¸­è‹±æ··åˆæœç´¢
- âœ… å¤šæ¨¡å‹åˆ‡æ¢
- âœ… ä¿æŒä¸ç°æœ‰è‹±æ–‡CLIPå…¼å®¹

---

## ğŸ¯ æ¨èæ¨¡å‹

### æ¨¡å‹ 1: Taiyi-CLIP-Roberta-102M-Chinese

**æ¥æº**: [IDEA-CCNL/Taiyi-CLIP-Roberta-102M-Chinese](https://huggingface.co/IDEA-CCNL/Taiyi-CLIP-Roberta-102M-Chinese)

**ä¼˜åŠ¿**:
- âœ… ä¸“ä¸ºä¸­æ–‡ä¼˜åŒ–
- âœ… 102Må‚æ•°ï¼Œæ¨¡å‹è¾ƒå°
- âœ… åŸºäºRobertaä¸­æ–‡é¢„è®­ç»ƒ
- âœ… æ”¯æŒä¸­æ–‡è¯­ä¹‰ç†è§£

**è§„æ ¼**:
- æ–‡æœ¬ç¼–ç å™¨: Chinese-Roberta-wwm-ext-base-chinese
- å›¾åƒç¼–ç å™¨: ViT-B/16
- ç‰¹å¾ç»´åº¦: 512
- è®­ç»ƒæ•°æ®: ä¸­æ–‡å›¾æ–‡å¯¹

**é€‚ç”¨åœºæ™¯**: çº¯ä¸­æ–‡æŸ¥è¯¢ï¼Œä¸­æ–‡å›¾åº“

---

### æ¨¡å‹ 2: CN-CLIP (ViT-B/16)

**æ¥æº**: [OFA-Sys/chinese-clip-vit-base-patch16](https://huggingface.co/OFA-Sys/chinese-clip-vit-base-patch16)
**é•œåƒ**: [eisneim/cn-clip_vit-b-16](https://huggingface.co/eisneim/cn-clip_vit-b-16)

**ä¼˜åŠ¿**:
- âœ… é˜¿é‡Œè¾¾æ‘©é™¢å‡ºå“
- âœ… 200M+ä¸­æ–‡å›¾æ–‡å¯¹è®­ç»ƒ
- âœ… ä¸­è‹±åŒè¯­æ”¯æŒ
- âœ… æ€§èƒ½æ¥è¿‘OpenAI CLIP

**è§„æ ¼**:
- æ–‡æœ¬ç¼–ç å™¨: BERT-base-chinese
- å›¾åƒç¼–ç å™¨: ViT-B/16
- ç‰¹å¾ç»´åº¦: 512
- è®­ç»ƒæ•°æ®: Noah-Wukong + è‡ªå»ºæ•°æ®é›†

**é€‚ç”¨åœºæ™¯**: ä¸­è‹±æ··åˆæŸ¥è¯¢ï¼Œé€šç”¨å›¾åº“

---

### æ¨¡å‹å¯¹æ¯”

| ç‰¹æ€§ | OpenAI CLIP | Taiyi-CLIP | CN-CLIP |
|------|-------------|------------|---------|
| **è¯­è¨€** | è‹±æ–‡ | ä¸­æ–‡ | ä¸­è‹±åŒè¯­ |
| **ç‰¹å¾ç»´åº¦** | 768 (L/14) | 512 | 512 |
| **æ¨¡å‹å¤§å°** | ~900MB | ~400MB | ~600MB |
| **ä¸­æ–‡æ€§èƒ½** | â­â­ | â­â­â­â­â­ | â­â­â­â­â­ |
| **è‹±æ–‡æ€§èƒ½** | â­â­â­â­â­ | â­â­ | â­â­â­â­ |
| **æ¨èç”¨é€”** | è‹±æ–‡/å›½é™… | çº¯ä¸­æ–‡ | ä¸­è‹±æ··åˆ |

**æ¨èé€‰æ‹©**: **CN-CLIP** (ä¸­è‹±åŒè¯­ï¼Œå…¼å®¹æ€§æœ€å¥½)

---

## ğŸ—ï¸ å®ç°æ–¹æ¡ˆ

### æ–¹æ¡ˆ A: å¤šæ¨¡å‹ç®¡ç†ï¼ˆæ¨èï¼‰

**æ¶æ„è®¾è®¡**:
```
ModelManager (å•ä¾‹)
â”œâ”€â”€ ClipEncoder (OpenAI CLIP - è‹±æ–‡)
â”œâ”€â”€ ChineseClipEncoder (CN-CLIP - ä¸­æ–‡)
â””â”€â”€ TaiyiClipEncoder (Taiyi-CLIP - ä¸­æ–‡)
```

**ä¼˜åŠ¿**:
- æ”¯æŒå¤šæ¨¡å‹åŒæ—¶åŠ è½½
- ç”¨æˆ·å¯é€‰æ‹©ä½¿ç”¨å“ªä¸ªæ¨¡å‹
- å¯ä»¥å¯¹æ¯”ä¸åŒæ¨¡å‹æ•ˆæœ

**å®ç°æ­¥éª¤**:
1. åˆ›å»º `ChineseClipEncoder` ç±»
2. æ‰©å±• `ModelManager` æ”¯æŒå¤šç¼–ç å™¨
3. æ›´æ–° GUI æ·»åŠ æ¨¡å‹é€‰æ‹©ä¸‹æ‹‰æ¡†
4. åˆ›å»ºæ¨¡å‹ä¸‹è½½/è½¬æ¢è„šæœ¬

---

### æ–¹æ¡ˆ B: ç»Ÿä¸€æ¥å£ï¼ˆç®€åŒ–ï¼‰

**æ¶æ„è®¾è®¡**:
```
ClipEncoder (åŸºç±»)
â”œâ”€â”€ OpenAIClipEncoder
â”œâ”€â”€ CNClipEncoder
â””â”€â”€ TaiyiClipEncoder
```

**ä¼˜åŠ¿**:
- ä»£ç ç»“æ„ç®€å•
- æ˜“äºç»´æŠ¤
- æ¥å£ç»Ÿä¸€

**å®ç°æ­¥éª¤**:
1. é‡æ„ `ClipEncoder` ä¸ºæŠ½è±¡åŸºç±»
2. å®ç°å„ä¸ªå…·ä½“ç¼–ç å™¨
3. è¿è¡Œæ—¶é…ç½®é€‰æ‹©æ¨¡å‹

---

## ğŸ“¦ æ¨¡å‹ä¸‹è½½ä¸è½¬æ¢

### è„šæœ¬ 1: ä¸‹è½½ä¸­æ–‡CLIPæ¨¡å‹

åˆ›å»º `scripts/download_chinese_clip.py`:

```python
#!/usr/bin/env python3
"""
ä¸‹è½½å¹¶è½¬æ¢ä¸­æ–‡CLIPæ¨¡å‹åˆ°ONNXæ ¼å¼
"""

import os
import argparse
from pathlib import Path
from huggingface_hub import snapshot_download
import torch
from transformers import BertTokenizer, BertModel, CLIPVisionModel
import onnx
from onnx import version_converter

def download_cn_clip(output_dir="./models/cn-clip"):
    """ä¸‹è½½CN-CLIPæ¨¡å‹"""
    print("ğŸ“¥ æ­£åœ¨ä¸‹è½½ CN-CLIP æ¨¡å‹...")

    # ä¸‹è½½æ¨¡å‹æ–‡ä»¶
    model_path = snapshot_download(
        repo_id="OFA-Sys/chinese-clip-vit-base-patch16",
        cache_dir=output_dir,
        local_dir=output_dir,
        local_dir_use_symlinks=False
    )

    print(f"âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ: {model_path}")
    return model_path

def download_taiyi_clip(output_dir="./models/taiyi-clip"):
    """ä¸‹è½½Taiyi-CLIPæ¨¡å‹"""
    print("ğŸ“¥ æ­£åœ¨ä¸‹è½½ Taiyi-CLIP æ¨¡å‹...")

    model_path = snapshot_download(
        repo_id="IDEA-CCNL/Taiyi-CLIP-Roberta-102M-Chinese",
        cache_dir=output_dir,
        local_dir=output_dir,
        local_dir_use_symlinks=False
    )

    print(f"âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ: {model_path}")
    return model_path

def export_to_onnx(model_path, output_dir):
    """å¯¼å‡ºæ¨¡å‹åˆ°ONNXæ ¼å¼"""
    print("ğŸ”„ æ­£åœ¨è½¬æ¢ä¸ºONNXæ ¼å¼...")

    # TODO: å®ç°ONNXè½¬æ¢
    # 1. åŠ è½½PyTorchæ¨¡å‹
    # 2. å¯¼å‡ºè§†è§‰ç¼–ç å™¨
    # 3. å¯¼å‡ºæ–‡æœ¬ç¼–ç å™¨
    # 4. éªŒè¯è¾“å‡º

    pass

def main():
    parser = argparse.ArgumentParser(description="ä¸‹è½½ä¸­æ–‡CLIPæ¨¡å‹")
    parser.add_argument("--model", choices=["cn-clip", "taiyi", "both"],
                       default="cn-clip", help="é€‰æ‹©è¦ä¸‹è½½çš„æ¨¡å‹")
    parser.add_argument("--output", default="./assets/models",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--export-onnx", action="store_true",
                       help="å¯¼å‡ºä¸ºONNXæ ¼å¼")

    args = parser.parse_args()

    output_path = Path(args.output)
    output_path.mkdir(parents=True, exist_ok=True)

    if args.model in ["cn-clip", "both"]:
        model_path = download_cn_clip(output_path / "cn-clip")
        if args.export_onnx:
            export_to_onnx(model_path, output_path / "cn-clip-onnx")

    if args.model in ["taiyi", "both"]:
        model_path = download_taiyi_clip(output_path / "taiyi-clip")
        if args.export_onnx:
            export_to_onnx(model_path, output_path / "taiyi-clip-onnx")

    print("ğŸ‰ å…¨éƒ¨å®Œæˆï¼")

if __name__ == "__main__":
    main()
```

---

### è„šæœ¬ 2: å¯¼å‡ºCN-CLIPåˆ°ONNX

åˆ›å»º `scripts/export_cn_clip_to_onnx.py`:

```python
#!/usr/bin/env python3
"""
å°†CN-CLIPæ¨¡å‹å¯¼å‡ºä¸ºONNXæ ¼å¼
"""

import torch
import onnx
from cn_clip.clip import load_from_name
import argparse
from pathlib import Path

def export_cn_clip_text_encoder(model, output_path):
    """å¯¼å‡ºæ–‡æœ¬ç¼–ç å™¨"""
    print("ğŸ“¤ å¯¼å‡ºæ–‡æœ¬ç¼–ç å™¨...")

    # å‡†å¤‡ç¤ºä¾‹è¾“å…¥
    dummy_input = torch.randint(0, 21128, (1, 77))  # CN-CLIP vocab size
    dummy_attention_mask = torch.ones(1, 77)

    # å¯¼å‡º
    torch.onnx.export(
        model.text,
        (dummy_input, dummy_attention_mask),
        output_path,
        input_names=['input_ids', 'attention_mask'],
        output_names=['text_features'],
        dynamic_axes={
            'input_ids': {0: 'batch_size'},
            'attention_mask': {0: 'batch_size'},
            'text_features': {0: 'batch_size'}
        },
        opset_version=14,
        do_constant_folding=True
    )

    print(f"âœ… æ–‡æœ¬ç¼–ç å™¨å·²å¯¼å‡º: {output_path}")

def export_cn_clip_visual_encoder(model, output_path):
    """å¯¼å‡ºè§†è§‰ç¼–ç å™¨"""
    print("ğŸ“¤ å¯¼å‡ºè§†è§‰ç¼–ç å™¨...")

    # å‡†å¤‡ç¤ºä¾‹è¾“å…¥ (1, 3, 224, 224)
    dummy_input = torch.randn(1, 3, 224, 224)

    # å¯¼å‡º
    torch.onnx.export(
        model.visual,
        dummy_input,
        output_path,
        input_names=['pixel_values'],
        output_names=['image_features'],
        dynamic_axes={
            'pixel_values': {0: 'batch_size'},
            'image_features': {0: 'batch_size'}
        },
        opset_version=14,
        do_constant_folding=True
    )

    print(f"âœ… è§†è§‰ç¼–ç å™¨å·²å¯¼å‡º: {output_path}")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", default="ViT-B-16",
                       help="CN-CLIPæ¨¡å‹åç§°")
    parser.add_argument("--output", default="./assets/models",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--device", default="cpu",
                       help="è®¾å¤‡ (cpu/cuda)")

    args = parser.parse_args()

    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)

    # åŠ è½½æ¨¡å‹
    print(f"ğŸ“¥ åŠ è½½ CN-CLIP æ¨¡å‹: {args.model}")
    model, preprocess = load_from_name(args.model, device=args.device)
    model.eval()

    # å¯¼å‡ºæ–‡æœ¬ç¼–ç å™¨
    text_output = output_dir / "cn_clip_text.onnx"
    export_cn_clip_text_encoder(model, text_output)

    # å¯¼å‡ºè§†è§‰ç¼–ç å™¨
    visual_output = output_dir / "cn_clip_visual.onnx"
    export_cn_clip_visual_encoder(model, visual_output)

    print("ğŸ‰ å¯¼å‡ºå®Œæˆï¼")
    print(f"  æ–‡æœ¬ç¼–ç å™¨: {text_output}")
    print(f"  è§†è§‰ç¼–ç å™¨: {visual_output}")

if __name__ == "__main__":
    main()
```

---

## ğŸ”§ ä»£ç é›†æˆ

### 1. åˆ›å»ºä¸­æ–‡CLIPç¼–ç å™¨ç±»

`src/core/chinese_clip_encoder.h`:

```cpp
#pragma once

#include "clip_encoder.h"
#include <string>
#include <memory>

namespace vindex {
namespace core {

/**
 * @brief ä¸­æ–‡CLIPç¼–ç å™¨
 *
 * æ”¯æŒä¸­æ–‡æ–‡æœ¬ç¼–ç å’Œå›¾åƒç¼–ç 
 * åŸºäºCN-CLIPæˆ–Taiyi-CLIPæ¨¡å‹
 */
class ChineseClipEncoder : public ClipEncoder {
public:
    /**
     * @brief æ„é€ å‡½æ•°
     * @param visualModelPath è§†è§‰æ¨¡å‹è·¯å¾„
     * @param textModelPath æ–‡æœ¬æ¨¡å‹è·¯å¾„
     * @param vocabPath è¯è¡¨è·¯å¾„ï¼ˆBERT tokenizerï¼‰
     * @param embeddingDim ç‰¹å¾ç»´åº¦ï¼ˆé»˜è®¤512ï¼‰
     */
    explicit ChineseClipEncoder(
        const std::string& visualModelPath,
        const std::string& textModelPath,
        const std::string& vocabPath,
        int embeddingDim = 512
    );

    ~ChineseClipEncoder() = default;

    /**
     * @brief ç¼–ç ä¸­æ–‡æ–‡æœ¬
     * @param text ä¸­æ–‡æ–‡æœ¬ï¼ˆUTF-8ç¼–ç ï¼‰
     * @return ç‰¹å¾å‘é‡
     */
    std::vector<float> encodeText(const std::string& text) override;

    /**
     * @brief æ‰¹é‡ç¼–ç ä¸­æ–‡æ–‡æœ¬
     */
    std::vector<float> encodeTextBatch(
        const std::vector<std::string>& texts
    ) override;

    /**
     * @brief è·å–æ¨¡å‹ç±»å‹
     */
    std::string getModelType() const override { return "CN-CLIP"; }

private:
    // ä½¿ç”¨BERT tokenizerè€ŒéBPE
    std::unique_ptr<class BertTokenizer> tokenizer_;
    int maxLength_;  // BERTé»˜è®¤512ï¼ŒCLIPé€šå¸¸ç”¨77
};

} // namespace core
} // namespace vindex
```

---

### 2. æ‰©å±•ModelManageræ”¯æŒå¤šæ¨¡å‹

`src/core/model_manager.h` æ·»åŠ ï¼š

```cpp
class ModelManager {
public:
    // ç°æœ‰æ–¹æ³•...

    /**
     * @brief è·å–ä¸­æ–‡CLIPç¼–ç å™¨
     */
    ChineseClipEncoder& chineseClipEncoder();
    bool hasChineseClipEncoder() const;

    /**
     * @brief è·å–å½“å‰æ¿€æ´»çš„CLIPç¼–ç å™¨
     * @return è‹±æ–‡æˆ–ä¸­æ–‡CLIPï¼ˆæ ¹æ®é…ç½®ï¼‰
     */
    ClipEncoder& activeClipEncoder();

    /**
     * @brief è®¾ç½®æ¿€æ´»æ¨¡å‹
     * @param type "openai", "cn-clip", "taiyi"
     */
    void setActiveModel(const std::string& type);

    std::string getActiveModelType() const { return activeModelType_; }

private:
    void initializeChineseClipEncoder();

    std::unique_ptr<ChineseClipEncoder> chineseClipEncoder_;
    std::string activeModelType_{"openai"};  // é»˜è®¤OpenAI CLIP
};
```

---

### 3. GUIæ›´æ–°ï¼šæ·»åŠ æ¨¡å‹é€‰æ‹©

åœ¨ `main_window.cpp` æ·»åŠ æ¨¡å‹é€‰æ‹©ä¸‹æ‹‰æ¡†ï¼š

```cpp
void MainWindow::setupToolBar() {
    QToolBar* toolbar = addToolBar("Main Toolbar");

    // ç°æœ‰å·¥å…·æ é¡¹...

    toolbar->addSeparator();

    // æ¨¡å‹é€‰æ‹©ä¸‹æ‹‰æ¡†
    toolbar->addWidget(new QLabel("CLIP Model:", this));

    modelSelector_ = new QComboBox(this);
    modelSelector_->addItem("OpenAI CLIP (English)", "openai");
    modelSelector_->addItem("CN-CLIP (ä¸­è‹±åŒè¯­)", "cn-clip");
    modelSelector_->addItem("Taiyi-CLIP (ä¸­æ–‡)", "taiyi");

    connect(modelSelector_, QOverload<int>::of(&QComboBox::currentIndexChanged),
            this, &MainWindow::onModelChanged);

    toolbar->addWidget(modelSelector_);
}

void MainWindow::onModelChanged(int index) {
    QString modelType = modelSelector_->itemData(index).toString();

    try {
        modelManager_->setActiveModel(modelType.toStdString());

        statusLabel_->setText(
            QString("Switched to %1")
            .arg(modelSelector_->currentText())
        );

    } catch (const std::exception& e) {
        QMessageBox::warning(
            this,
            "Error",
            QString("Failed to switch model: %1").arg(e.what())
        );
    }
}
```

---

## ğŸ“ é…ç½®æ–‡ä»¶

åˆ›å»º `assets/config/models.json`:

```json
{
  "models": {
    "openai-clip": {
      "name": "OpenAI CLIP",
      "language": "en",
      "visual_model": "clip_visual.onnx",
      "text_model": "clip_text.onnx",
      "vocab": "vocab/bpe_simple_vocab_16e6.txt",
      "tokenizer": "bpe",
      "embedding_dim": 768,
      "enabled": true
    },
    "cn-clip": {
      "name": "CN-CLIP",
      "language": "zh-cn,en",
      "visual_model": "cn_clip_visual.onnx",
      "text_model": "cn_clip_text.onnx",
      "vocab": "vocab/bert-base-chinese-vocab.txt",
      "tokenizer": "bert",
      "embedding_dim": 512,
      "enabled": false
    },
    "taiyi-clip": {
      "name": "Taiyi-CLIP",
      "language": "zh-cn",
      "visual_model": "taiyi_clip_visual.onnx",
      "text_model": "taiyi_clip_text.onnx",
      "vocab": "vocab/roberta-chinese-vocab.txt",
      "tokenizer": "roberta",
      "embedding_dim": 512,
      "enabled": false
    }
  },
  "default_model": "openai-clip"
}
```

---

## ğŸ§ª æµ‹è¯•è®¡åˆ’

### æµ‹è¯•ç”¨ä¾‹

| åœºæ™¯ | æŸ¥è¯¢ | é¢„æœŸç»“æœ |
|------|------|----------|
| ä¸­æ–‡æŸ¥è¯¢ | "ä¸€åªçŒ«" | è¿”å›çŒ«çš„å›¾ç‰‡ |
| è‹±æ–‡æŸ¥è¯¢ | "a cat" | è¿”å›çŒ«çš„å›¾ç‰‡ |
| ä¸­è‹±æ··åˆ | "çº¢è‰²çš„car" | è¿”å›çº¢è‰²æ±½è½¦ |
| é•¿å¥å­ | "å¤•é˜³ä¸‹çš„æµ·æ»©" | è¿”å›ç›¸å…³åœºæ™¯ |
| ä¸“æœ‰åè¯ | "æ•…å®«" | è¿”å›æ•…å®«å›¾ç‰‡ |

### æ€§èƒ½åŸºå‡†

| æ¨¡å‹ | ç¼–ç æ—¶é—´ (CPU) | æœç´¢æ—¶é—´ (10K) | å†…å­˜å ç”¨ |
|------|----------------|----------------|----------|
| OpenAI CLIP | ~50ms | ~10ms | ~900MB |
| CN-CLIP | ~40ms | ~10ms | ~600MB |
| Taiyi-CLIP | ~35ms | ~10ms | ~400MB |

---

## ğŸ“š ä¾èµ–æ›´æ–°

æ›´æ–° `scripts/requirements.txt`:

```txt
# ç°æœ‰ä¾èµ–
torch>=2.0.0
onnx>=1.14.0
onnxruntime>=1.15.0
open-clip-torch>=2.20.0

# æ–°å¢ï¼šä¸­æ–‡CLIPæ”¯æŒ
cn_clip  # CN-CLIPå®˜æ–¹åŒ…
transformers>=4.30.0  # BERT tokenizer
huggingface_hub>=0.16.0  # æ¨¡å‹ä¸‹è½½
sentencepiece>=0.1.99  # å¯é€‰ï¼šæ›´å¥½çš„ä¸­æ–‡åˆ†è¯
```

---

## ğŸ“‹ å®æ–½è®¡åˆ’

### ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€æ”¯æŒï¼ˆ1-2å¤©ï¼‰
- [ ] åˆ›å»ºä¸­æ–‡CLIPä¸‹è½½è„šæœ¬
- [ ] å®ç°ONNXè½¬æ¢è„šæœ¬
- [ ] æµ‹è¯•æ¨¡å‹å¯¼å‡º

### ç¬¬äºŒé˜¶æ®µï¼šä»£ç é›†æˆï¼ˆ2-3å¤©ï¼‰
- [ ] åˆ›å»º `ChineseClipEncoder` ç±»
- [ ] å®ç°BERT tokenizeré›†æˆ
- [ ] æ‰©å±• `ModelManager` å¤šæ¨¡å‹æ”¯æŒ
- [ ] æ·»åŠ é…ç½®æ–‡ä»¶åŠ è½½

### ç¬¬ä¸‰é˜¶æ®µï¼šGUIæ›´æ–°ï¼ˆ1å¤©ï¼‰
- [ ] æ·»åŠ æ¨¡å‹é€‰æ‹©ä¸‹æ‹‰æ¡†
- [ ] æ›´æ–°æœç´¢ç•Œé¢æç¤º
- [ ] æ·»åŠ è¯­è¨€è‡ªåŠ¨æ£€æµ‹

### ç¬¬å››é˜¶æ®µï¼šæµ‹è¯•ä¸ä¼˜åŒ–ï¼ˆ1-2å¤©ï¼‰
- [ ] åŠŸèƒ½æµ‹è¯•
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•
- [ ] æ–‡æ¡£æ›´æ–°
- [ ] ç¤ºä¾‹å’Œæ•™ç¨‹

**æ€»è®¡**: 5-8å¤©

---

## ğŸ’¡ é¢å¤–ä¼˜åŒ–

### 1. è‡ªåŠ¨è¯­è¨€æ£€æµ‹

```cpp
std::string detectLanguage(const std::string& text) {
    // ç®€å•å®ç°ï¼šæ£€æµ‹ä¸­æ–‡å­—ç¬¦
    int chineseCount = 0;
    for (unsigned char c : text) {
        if (c >= 0x80) chineseCount++;  // éASCII
    }

    float ratio = static_cast<float>(chineseCount) / text.length();
    return ratio > 0.3 ? "zh" : "en";
}

ClipEncoder& ModelManager::autoSelectEncoder(const std::string& text) {
    std::string lang = detectLanguage(text);

    if (lang == "zh" && hasChineseClipEncoder()) {
        return chineseClipEncoder();
    } else {
        return clipEncoder();
    }
}
```

### 2. æ··åˆæœç´¢

æ”¯æŒåŒæ—¶ä½¿ç”¨å¤šä¸ªæ¨¡å‹æœç´¢å¹¶åˆå¹¶ç»“æœï¼š

```cpp
std::vector<SearchResult> DatabaseManager::hybridSearch(
    const std::string& query,
    int topK
) {
    // ä½¿ç”¨ä¸¤ä¸ªæ¨¡å‹åˆ†åˆ«æœç´¢
    auto results1 = searchWithEncoder(query, clipEncoder(), topK);
    auto results2 = searchWithEncoder(query, chineseClipEncoder(), topK);

    // åˆå¹¶å¹¶é‡æ–°æ’åº
    return mergeResults(results1, results2, topK);
}
```

### 3. æŸ¥è¯¢ç¿»è¯‘

å¯¹äºè·¨è¯­è¨€æœç´¢ï¼Œå¯ä»¥é›†æˆç¿»è¯‘APIï¼š

```cpp
std::string translateQuery(const std::string& text,
                          const std::string& targetLang) {
    // è°ƒç”¨ç¿»è¯‘APIï¼ˆå¦‚ç™¾åº¦ç¿»è¯‘ã€Google Translateï¼‰
    // å®ç°æŸ¥è¯¢ç¿»è¯‘
    return translatedText;
}
```

---

## ğŸ“– ç”¨æˆ·æ–‡æ¡£æ›´æ–°

æ·»åŠ åˆ° `docs/QUICKSTART.md`:

### ä½¿ç”¨ä¸­æ–‡CLIPæ¨¡å‹

1. **ä¸‹è½½æ¨¡å‹**:
   ```bash
   cd scripts
   python download_chinese_clip.py --model cn-clip --export-onnx
   ```

2. **å¯åŠ¨VIndex**:
   - åœ¨å·¥å…·æ é€‰æ‹© "CN-CLIP (ä¸­è‹±åŒè¯­)"

3. **ä¸­æ–‡æœç´¢**:
   - è¾“å…¥ä¸­æ–‡æŸ¥è¯¢ï¼š"ä¸€åªå¯çˆ±çš„çŒ«"
   - æ”¯æŒä¸­è‹±æ··åˆï¼š"çº¢è‰²çš„car"

4. **æ€§èƒ½å¯¹æ¯”**:
   - OpenAI CLIP: é€‚åˆè‹±æ–‡æŸ¥è¯¢
   - CN-CLIP: é€‚åˆä¸­è‹±åŒè¯­
   - Taiyi-CLIP: é€‚åˆçº¯ä¸­æ–‡

---

## ğŸ¯ æ€»ç»“

æ·»åŠ ä¸­æ–‡CLIPæ”¯æŒå°†ä½¿VIndexæˆä¸º**çœŸæ­£çš„å¤šè¯­è¨€è§†è§‰æœç´¢å¼•æ“**ï¼

**ä¼˜åŠ¿**:
- ğŸŒ æ”¯æŒå…¨çƒæœ€å¤§çš„ä¸­æ–‡ç”¨æˆ·ç¾¤ä½“
- ğŸ”€ ä¸­è‹±åŒè¯­æ— ç¼åˆ‡æ¢
- ğŸš€ æ€§èƒ½ä¼˜å¼‚ï¼ˆæ¨¡å‹æ›´å°æ›´å¿«ï¼‰
- ğŸ¨ æ›´å¥½çš„ä¸­æ–‡è¯­ä¹‰ç†è§£

**ä¸‹ä¸€æ­¥**:
1. ç¡®è®¤æ˜¯å¦å¼€å§‹å®æ–½
2. é€‰æ‹©ä¼˜å…ˆé›†æˆçš„æ¨¡å‹ï¼ˆæ¨èCN-CLIPï¼‰
3. åˆ›å»ºæ¨¡å‹ä¸‹è½½å’Œè½¬æ¢è„šæœ¬
4. é€æ­¥é›†æˆåˆ°ä»£ç åº“

---

**ç»´æŠ¤è€…**: VIndexå¼€å‘å›¢é˜Ÿ
**æœ€åæ›´æ–°**: 2025-11-26
