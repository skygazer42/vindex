# ğŸ”´ ä¸¥é‡Bugä¿®å¤ - CN-CLIPé›†æˆ

**æ—¥æœŸ**: 2025-11-26
**ä¼˜å…ˆçº§**: **ğŸš¨ ç«‹å³ä¿®å¤**

---

## é—®é¢˜æè¿°

### Bug 1: ä¸Šä¸‹æ–‡é•¿åº¦é”™è¯¯ ğŸ”´

**ä½ç½®**: `src/core/clip_encoder.cpp` ç¬¬22-28è¡Œ

**å½“å‰ä»£ç ** (âŒ é”™è¯¯):
```cpp
// CN-CLIP æ–‡æœ¬é•¿åº¦é€šå¸¸ä¸º512ï¼Œæ ‡å‡† CLIP ä¸º77
int contextLen = 77;
std::string lowerPath = textModelPath;
std::transform(lowerPath.begin(), lowerPath.end(), lowerPath.begin(), ::tolower);
if (lowerPath.find("cn-clip") != std::string::npos || lowerPath.find("vit-b-16") != std::string::npos) {
    contextLen = 512;  // âŒ é”™è¯¯ï¼åº”è¯¥æ˜¯52
}
textTokenizer_ = std::make_unique<TextTokenizer>(vocabPath, contextLen);
```

**é—®é¢˜**:
- ä»£ç è®¾ç½® contextLen = 512
- ä½†eisneim CN-CLIPå®é™…éœ€è¦ **52 tokens**
- è¿™ä¼šå¯¼è‡´ç»´åº¦ä¸åŒ¹é…ï¼Œæ¨ç†å¤±è´¥

**æ­£ç¡®ä»£ç ** (âœ… ä¿®å¤):
```cpp
// CN-CLIP (eisneim) æ–‡æœ¬é•¿åº¦ä¸º52ï¼Œæ ‡å‡† CLIP ä¸º77
int contextLen = 77;
std::string lowerPath = textModelPath;
std::transform(lowerPath.begin(), lowerPath.end(), lowerPath.begin(), ::tolower);
if (lowerPath.find("cn-clip") != std::string::npos || lowerPath.find("eisneim") != std::string::npos) {
    contextLen = 52;  // âœ… eisneim CN-CLIPä½¿ç”¨52ä¸ªtoken
} else if (lowerPath.find("vit-b-16") != std::string::npos && lowerPath.find("cn") != std::string::npos) {
    contextLen = 52;  // âœ… å…¶ä»–CN-CLIPå˜ä½“ä¹Ÿå¯èƒ½ä½¿ç”¨52
}
textTokenizer_ = std::make_unique<TextTokenizer>(vocabPath, contextLen);
```

---

### Bug 2: æ³¨é‡Šé”™è¯¯

**ä½ç½®**: `src/core/model_manager.cpp` ç¬¬14è¡Œ

**å½“å‰ä»£ç ** (âŒ é”™è¯¯):
```cpp
, embeddingDim_(512)  // é»˜è®¤åŒ¹é…ä¸­æ–‡ CN-CLIP (vit-b-16 è¾“å‡º512ç»´)
```

**é—®é¢˜**:
- æ³¨é‡Šæ­£ç¡®ï¼ˆ512ç»´embeddingæ˜¯å¯¹çš„ï¼‰
- ä½†ä¸è¦ä¸tokenizerçš„context_lengthæ··æ·†

**æ­£ç¡®ä»£ç ** (âœ… æ”¹è¿›):
```cpp
, embeddingDim_(512)  // CN-CLIPç‰¹å¾ç»´åº¦512ï¼ˆæ³¨æ„ï¼šä¸æ˜¯context lengthï¼‰
```

---

## éªŒè¯æ•°æ®

### eisneim CN-CLIPå®é™…è¾“å…¥è¦æ±‚

```python
# å›¾åƒç¼–ç å™¨
è¾“å…¥: image [1, 3, 224, 224] (tensor(float))
è¾“å‡º: unnorm_image_features [1, 512] (tensor(float))

# æ–‡æœ¬ç¼–ç å™¨ âš ï¸ æ³¨æ„è¿™é‡Œ
è¾“å…¥: text [1, 52] (tensor(int64))  # â† 52ä¸ªtokenï¼Œä¸æ˜¯77æˆ–512ï¼
è¾“å‡º: unnorm_text_features [1, 512] (tensor(float))
```

### ä¸åŒæ¨¡å‹å¯¹æ¯”

| æ¨¡å‹ | Context Length | Embedding Dim |
|------|----------------|---------------|
| **OpenAI CLIP** | 77 | 768 |
| **eisneim CN-CLIP** | **52** âš ï¸ | 512 |
| **OFA CN-CLIP** | 77 (å¯èƒ½) | 512 |
| **Taiyi-CLIP** | 77 (å¯èƒ½) | 512 |

---

## ä¿®å¤æ­¥éª¤

### æ­¥éª¤1: ä¿®å¤ä¸Šä¸‹æ–‡é•¿åº¦

**æ–‡ä»¶**: `src/core/clip_encoder.cpp`

```cpp
// ç¬¬22-29è¡Œï¼Œä¿®æ”¹ä¸ºï¼š
if (!textModelPath.empty() && !vocabPath.empty()) {
    // eisneim CN-CLIPä½¿ç”¨52 tokensï¼Œæ ‡å‡†CLIPä½¿ç”¨77
    int contextLen = 77;
    std::string lowerPath = textModelPath;
    std::transform(lowerPath.begin(), lowerPath.end(), lowerPath.begin(), ::tolower);

    // æ£€æµ‹eisneim CN-CLIPæ¨¡å‹
    if (lowerPath.find("eisneim") != std::string::npos ||
        lowerPath.find("vit-b-16.txt") != std::string::npos) {
        contextLen = 52;  // eisneim CN-CLIPç‰¹æ®Šé•¿åº¦
    }

    textTokenizer_ = std::make_unique<TextTokenizer>(vocabPath, contextLen);
}
```

---

### æ­¥éª¤2: æ›´æ–°æ³¨é‡Š

**æ–‡ä»¶**: `src/core/model_manager.cpp`

```cpp
// ç¬¬14è¡Œï¼Œæ›´æ–°æ³¨é‡Šï¼š
, embeddingDim_(512)  // CN-CLIP embeddingç»´åº¦512 (context lengthå¦å¤–é…ç½®)
```

---

### æ­¥éª¤3: æ›´æ–°é…ç½®æ–‡ä»¶æ³¨é‡Š

**æ–‡ä»¶**: `assets/config/app_config.json`

```json
{
  "models": {
    "clip": {
      "visual_model": "assets/models/cn-clip-eisneim/vit-b-16.img.fp32.onnx",
      "text_model": "assets/models/cn-clip-eisneim/vit-b-16.txt.fp32.onnx",
      "vocab_path": "assets/vocab/clip_vocab.txt",
      "embedding_dim": 512,
      "context_length": 52,  // â† æ·»åŠ è¿™ä¸ªè¯´æ˜
      "model_name": "CN-CLIP-ViT-B-16 (eisneim)",
      "note": "eisneimç‰ˆæœ¬ä½¿ç”¨52 tokensï¼Œä¸æ˜¯æ ‡å‡†çš„77"
    }
  }
}
```

---

## æµ‹è¯•éªŒè¯

### æµ‹è¯•1: ç¼–è¯‘æµ‹è¯•

```bash
cd /data/temp34/vindex/build
cmake ..
make -j$(nproc)

# åº”è¯¥æ— ç¼–è¯‘é”™è¯¯
```

### æµ‹è¯•2: ç®€å•æ¨ç†æµ‹è¯•

```cpp
// åˆ›å»ºæµ‹è¯•æ–‡ä»¶ test_cn_clip.cpp
#include "core/model_manager.h"
#include <iostream>

int main() {
    auto& modelManager = vindex::core::ModelManager::instance();

    modelManager.setModelPath("./assets/models");
    modelManager.setVocabPath("./assets/vocab/clip_vocab.txt");
    modelManager.setEmbeddingDim(512);

    auto& encoder = modelManager.clipEncoder();

    // æµ‹è¯•æ–‡æœ¬ç¼–ç 
    try {
        std::cout << "æµ‹è¯•ä¸­æ–‡æ–‡æœ¬ç¼–ç ..." << std::endl;
        auto features = encoder.encodeText("ä¸€åªå¯çˆ±çš„çŒ«å’ª");
        std::cout << "âœ… æˆåŠŸï¼ç‰¹å¾ç»´åº¦: " << features.size() << std::endl;

        if (features.size() != 512) {
            std::cerr << "âŒ é”™è¯¯ï¼šæœŸæœ›512ç»´ï¼Œå®é™…" << features.size() << "ç»´" << std::endl;
            return 1;
        }

        std::cout << "âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼" << std::endl;
        return 0;

    } catch (const std::exception& e) {
        std::cerr << "âŒ é”™è¯¯: " << e.what() << std::endl;
        return 1;
    }
}
```

### æµ‹è¯•3: ç«¯åˆ°ç«¯æ–‡æœå›¾æµ‹è¯•

```bash
# å¯åŠ¨VIndex
./VIndex

# åœ¨GUIä¸­ï¼š
# 1. åˆ‡æ¢åˆ° Text Search æ ‡ç­¾é¡µ
# 2. è¾“å…¥æŸ¥è¯¢ï¼š"ä¸€åªçŒ«"
# 3. ç‚¹å‡» Search
# 4. æ£€æŸ¥æ˜¯å¦è¿”å›ç»“æœ

# æœŸæœ›ï¼š
# - æ— é”™è¯¯æç¤º
# - è¿”å›ç›¸å…³å›¾ç‰‡
# - ç›¸ä¼¼åº¦åˆ†æ•°æ­£å¸¸ï¼ˆ0.0-1.0ï¼‰
```

---

## æ ¹æœ¬åŸå› åˆ†æ

### ä¸ºä»€ä¹ˆä¼šå‡ºç°è¿™ä¸ªbugï¼Ÿ

1. **è¯¯è§£æ¨¡å‹è§„æ ¼**
   - å¯èƒ½çœ‹åˆ°512ç»´embeddingï¼Œè¯¯è®¤ä¸ºcontext lengthä¹Ÿæ˜¯512
   - å®é™…ä¸Šembeddingç»´åº¦å’Œcontext lengthæ˜¯ä¸¤ä¸ªä¸åŒçš„æ¦‚å¿µ

2. **ç¼ºå°‘æ¨¡å‹æ–‡æ¡£æ£€æŸ¥**
   - åº”è¯¥å…ˆç”¨Pythonæ£€æŸ¥ONNXæ¨¡å‹çš„å®é™…è¾“å…¥å½¢çŠ¶
   - å†æ ¹æ®å®é™…å½¢çŠ¶ç¼–å†™ä»£ç 

3. **ä¸åŒç‰ˆæœ¬çš„å·®å¼‚**
   - eisneimç‰ˆæœ¬é’ˆå¯¹ä¸­æ–‡ä¼˜åŒ–ï¼Œä½¿ç”¨æ›´çŸ­çš„context length (52)
   - æ ‡å‡†CLIPä½¿ç”¨77
   - OpenAI CLIP-ViT-Lä½¿ç”¨77

### æ­£ç¡®çš„å¼€å‘æµç¨‹

1. **å…ˆéªŒè¯æ¨¡å‹** âœ…
   ```python
   import onnxruntime as ort
   sess = ort.InferenceSession("model.onnx")
   for inp in sess.get_inputs():
       print(f"{inp.name}: {inp.shape}")
   ```

2. **å†ç¼–å†™ä»£ç ** âœ…
   - æ ¹æ®å®é™…è¾“å…¥å½¢çŠ¶é…ç½®
   - æ·»åŠ è¯¦ç»†æ³¨é‡Šè¯´æ˜

3. **æœ€åæµ‹è¯•éªŒè¯** âœ…
   - å•å…ƒæµ‹è¯•
   - é›†æˆæµ‹è¯•
   - ç«¯åˆ°ç«¯æµ‹è¯•

---

## é¢„é˜²æªæ–½

### 1. æ·»åŠ è¿è¡Œæ—¶æ£€æŸ¥

åœ¨ `ClipEncoder::initializeSessions()` ä¸­æ·»åŠ ï¼š

```cpp
// éªŒè¯æ–‡æœ¬æ¨¡å‹è¾“å…¥å½¢çŠ¶
if (textSession_) {
    Ort::AllocatorWithDefaultOptions allocator;
    auto inputTypeInfo = textSession_->GetInputTypeInfo(0);
    auto tensorInfo = inputTypeInfo.GetTensorTypeAndShapeInfo();
    auto shape = tensorInfo.GetShape();

    if (shape.size() >= 2) {
        int64_t expectedLen = shape[1];
        int64_t actualLen = textTokenizer_->getContextLength();

        if (expectedLen > 0 && expectedLen != actualLen) {
            std::cerr << "âš ï¸  è­¦å‘Šï¼šTokenizeré•¿åº¦(" << actualLen
                      << ")ä¸æ¨¡å‹æœŸæœ›(" << expectedLen << ")ä¸åŒ¹é…ï¼" << std::endl;

            // å¯é€‰ï¼šè‡ªåŠ¨è°ƒæ•´æˆ–æŠ›å‡ºå¼‚å¸¸
            throw std::runtime_error(
                "Context length mismatch: tokenizer=" + std::to_string(actualLen) +
                ", model=" + std::to_string(expectedLen)
            );
        }
    }
}
```

### 2. æ·»åŠ æ–‡æ¡£æ³¨é‡Š

åœ¨å…³é”®ä½ç½®æ·»åŠ æ¸…æ™°çš„æ³¨é‡Šï¼š

```cpp
// âš ï¸ æ³¨æ„ï¼šä¸åŒCLIPæ¨¡å‹çš„context lengthä¸åŒï¼
//   - OpenAI CLIP: 77 tokens
//   - eisneim CN-CLIP: 52 tokens  â† ç‰¹æ®Šï¼
//   - å…¶ä»–CN-CLIPå¯èƒ½: 77 tokens
// è¯·æ ¹æ®å®é™…æ¨¡å‹éªŒè¯åé…ç½®
int contextLen = 77;  // é»˜è®¤å€¼
```

---

## ä¿®å¤åçš„æ•ˆæœ

### ä¿®å¤å‰ (âŒ é”™è¯¯)
```
é”™è¯¯: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT :
Got invalid dimensions for input: text for the following indices
 index: 1 Got: 512 Expected: 52
```

### ä¿®å¤å (âœ… æ­£å¸¸)
```
âœ… æ–‡æœ¬ç¼–ç æˆåŠŸ
âœ… ç‰¹å¾ç»´åº¦: 512
âœ… ç›¸ä¼¼åº¦è®¡ç®—æ­£å¸¸
âœ… æ–‡æœå›¾åŠŸèƒ½å¯ç”¨
```

---

## æ£€æŸ¥æ¸…å•

ä¿®å¤å‰è¯·ç¡®è®¤ï¼š

- [ ] å·²ç†è§£é—®é¢˜æ ¹æºï¼ˆ512 vs 52ï¼‰
- [ ] å·²ä¿®æ”¹ clip_encoder.cpp ç¬¬27è¡Œ
- [ ] å·²æ›´æ–°ç›¸å…³æ³¨é‡Š
- [ ] å·²æ·»åŠ è¿è¡Œæ—¶æ£€æŸ¥ï¼ˆå¯é€‰ï¼‰
- [ ] é‡æ–°ç¼–è¯‘é¡¹ç›®
- [ ] è¿è¡Œæµ‹è¯•éªŒè¯
- [ ] æ›´æ–°ç›¸å…³æ–‡æ¡£

---

## å…¶ä»–æ­£ç¡®çš„é…ç½®

### âœ… è¿™äº›é…ç½®æ˜¯æ­£ç¡®çš„

1. **Embeddingç»´åº¦ = 512** âœ…
   - CN-CLIPè¾“å‡º512ç»´ç‰¹å¾å‘é‡
   - è¿™ä¸ªæ˜¯æ­£ç¡®çš„

2. **Attention maskç”Ÿæˆ** âœ…
   - ä»£ç ä¸­çš„attention maskç”Ÿæˆé€»è¾‘æ­£ç¡®
   - éé›¶tokenè®¾ä¸º1ï¼Œé›¶tokenè®¾ä¸º0

3. **åŒè¾“å…¥æ”¯æŒ** âœ…
   - æ”¯æŒ input_ids + attention_mask
   - é€»è¾‘æ­£ç¡®

4. **L2å½’ä¸€åŒ–** âœ…
   - ç‰¹å¾å‘é‡å½’ä¸€åŒ–
   - å®ç°æ­£ç¡®

5. **æ¨¡å‹è·¯å¾„** âœ…
   - eisneim ONNXè·¯å¾„é…ç½®æ­£ç¡®
   - å›é€€æœºåˆ¶åˆç†

### âŒ åªæœ‰è¿™ä¸€ä¸ªéœ€è¦ä¿®å¤

- **Context length: 512 â†’ 52** âŒ
  - è¿™æ˜¯å”¯ä¸€çš„å…³é”®bug
  - å¿…é¡»ç«‹å³ä¿®å¤

---

## æ€»ç»“

**é—®é¢˜**: Context lengthè®¾ç½®ä¸º512ï¼Œä½†æ¨¡å‹éœ€è¦52

**å½±å“**:
- ğŸ”´ **ä¸¥é‡** - æ–‡æœ¬ç¼–ç å®Œå…¨æ— æ³•å·¥ä½œ
- ğŸ”´ æ¨ç†ä¼šç«‹å³å¤±è´¥
- ğŸ”´ æ–‡æœå›¾åŠŸèƒ½ä¸å¯ç”¨

**ä¿®å¤**:
- ç®€å• - åªéœ€æ”¹ä¸€ä¸ªæ•°å­—ï¼š512 â†’ 52
- å¿«é€Ÿ - 5åˆ†é’Ÿå³å¯å®Œæˆ
- å…³é”® - ä¿®å¤ååŠŸèƒ½ç«‹å³å¯ç”¨

**éªŒè¯**:
- ç¼–è¯‘æ— è¯¯
- æ¨ç†æˆåŠŸ
- æ–‡æœå›¾å¯ç”¨

---

**ç»“è®º**: ä»£ç æ•´ä½“æ¶æ„å’Œå®ç°éƒ½å¾ˆå¥½ï¼Œåªæœ‰è¿™**ä¸€ä¸ªæ•°å­—**éœ€è¦ä¿®å¤ï¼ä¿®å¤åå³å¯æ­£å¸¸ä½¿ç”¨CN-CLIPè¿›è¡Œä¸­æ–‡æ–‡æœå›¾ã€‚

---

**ç»´æŠ¤è€…**: VIndexå¼€å‘å›¢é˜Ÿ
**æœ€åæ›´æ–°**: 2025-11-26
